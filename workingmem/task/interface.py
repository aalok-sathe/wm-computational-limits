from abc import ABC, abstractmethod
import typing
from pathlib import Path
import yaml
from hashlib import sha1

import torch


class GeneratedCachedDataset(ABC, torch.utils.data.Dataset):
    """
    abstract class for a dataset that is generated and cached on disk. the dataset
    has three splits: train, test, valid. the dataset is generated by the `generate`
    and cached by the `cache` method.
    a generated dataset is loaded by the `from_path` classmethod which returns an instance of the
    dataset using the config available at the path.
    the dataset object has `attrs` which specifies the parameters used to generate the dataset.

    """

    _hash_length = 5
    attrs: dict

    def __init__(self, basedir="datasets", split=None, **kwargs):
        self.attrs = kwargs
        self.basedir = Path(basedir).expanduser().resolve()

        train_path = self.basedir / "train.json"
        eval_path = self.basedir / "eval.json"
        test_path = self.basedir / "test.json"
        if train_path.exists() and eval_path.exists() and test_path.exists():
            # list contents of the directory
            print(f"data already exists at {self.basedir}: ", [*self.basedir.iterdir()])
        else:
            self.generate()

    def __len__(self):
        return self.attrs[f"n_{self.split}"]

    @abstractmethod
    def __getitem__(self, index):
        pass

    @abstractmethod
    def generate(self):
        pass

    @classmethod
    def from_path(
        cls, path: typing.Union[str, Path], basedir="datasets", split="train"
    ):
        """
        creates a dataset instance from a path to a stored dataset

        Args:
        ---
        path (typing.Union[str, Path]):
            1. path to the stored dataset
            2. string path to the stored dataset
            3. hash string identifier of the dataset (e.g. XY7ZY)

        basedir (str):
            the base directory to store the dataset in (default: 'datasets/')

        split (str):
            the split of the dataset to use. if no data already exists on disk,
            data is generated for all splits (we need to make sure all examples
            are unique and non-repeating across splits). if data already exists
            (or once data has been generated), simply supplies examples from
            appropriate split. defaults to "train".
        """
        if isinstance(path, str):
            dest = Path(path).expanduser().resolve()
            if not dest.exists():
                dest = Path()
        config = yaml.load(dest / "config.yaml", Loader=yaml.SafeLoader)
        instance = cls(**config)
        return instance

    def __str__(self):
        """
        creates a stringified identity of the dataset for storing on disk
        """
        attr_str = ",".join([f"{k}={v}" for k, v in self.attrs.items()])
        H = sha1(attr_str.encode()).hexdigest()[: self._hash_length]
        return f"{self.__class__.__name__}_{H}_({attr_str})"

    def _metadata(self):
        """
        returns a dictionary of metadata for the dataset, which will (ideally)
        be dumped as a YAML file alongside the dataset in the dataset root
        """
        return self.attrs

    def __eq__(self, other):
        """
        we consider two datasets equivalent (not equal) if they share many of the
        attributes that characterizes the task demand
        """
        return str(self) == str(other)
